---
title: "ADS"
author: "Yoan Charpentier - 5A IMDS"
date: "2024-06-30"
output: 
  bookdown::html_document2 :
    code_folding: hide
    theme: lumen
    df_print: paged
    highlight: pygments
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      position: fixed
bibliography: references.bib
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

------------------------------------------------------------------------

```{=html}
    <center><img src="PolytechClermont_logo.jpg" alt="Logo" style="width:350px; float:left; margin-right:15px;"/></center>
    <center><img src="Logo_Clermont_Auvergne_INP.png" alt="Logo" style="width:125px; float:left; margin-right:15px;"/></center>
    
```
```{=html}
   <style>
        /* Style pour le grand titre */
        .grand-titre {
            font-size: 2.5em; /* Taille du texte */
            font-weight: bold; /* Texte en gras */
            color: #2C3E50; /* Couleur principale (bleu foncé) */
            text-align: center; /* Centrer le texte */
            margin-top: 200px; /* Espacement au-dessus */
            margin-bottom: 10px; /* Espacement en-dessous */
            text-transform: uppercase; /* Mettre tout en majuscules */
            letter-spacing: 2px; /* Espacement entre les lettres */
            background: linear-gradient(to right, #3498DB, #9B59B6); /* Dégradé de couleurs */
            -webkit-background-clip: text; /* Utiliser le dégradé pour le texte */
            -webkit-text-fill-color: transparent; /* Rendre le fond du texte transparent */
        }
    </style>

     <h1 class="grand-titre">Cohorte sur l'insuffisance cardiaque, une étude applicative mathématiques</h1>
     
```
# Introduction et motivation

L'insuffisance cardiaque désigne un disfonctionnement du muscle cardiaque caractérisée par une perte: de sa force musculaire et de sa capacité de contraction normale. Cette **complication** touche essentiellement une population "âgée" (i.e \>75 ans), en effet, 10 à 20% d'entre eux en seraient touchés. [@sante_publique_france_2019] Sans rentrer dans les détails, l'insuffisance cardiaque s'observe souvent par un essoufflement rapide à l'effort, une respiration courte et sifflante, de la toux et des palpitations cardiaques. Ces observations souligne la necessitée d'une compréhension statistiques de l'étiologie de l'insuffisance cardiaque [@vidal_insuffisance_cardiaque]

Ce rapport a donc pour objet l'analyse des données de patients atteint d'une insuffisance cardiaque. On s'appuie sur une base de données issue d'une étude de suivie (cohorte) de 299 patients atteints de d'insuffisance cardiaque.

Cette étude détaillée aura pour but d'explorer la survie de patients atteint d'insuffisance cardiaque. Pour cela on a à notre disposition une cohorte de sur 299 patients sur 285 jours [@Chicco2020].

On se demande alors :

-   Peut-on approximer la survie de ces patients ?

-   Quels sont les facteurs qui augmente le risque de décès lors d'une insuffisance cardiaque ?

Après une analyse descriptive des données, nous ferons un rappel mathématiques qui nous permettra de débuter une analyse de survie sans covariable. Ensuite, on cherchera a approcher l'estimation de la survie par des lois connues. On étudiera enfin le modèle en présence des covariables. Nous concluerons par un avis d'une étudiante en 5ème année de pharmacie sur les conclusions statistiques tirées.

# Analyse exploratoire, étude descriptive

Le jeu de données observe 299 patients atteints d'une insuffisance cardiaque dans une étude de 285 jours. Sur ces patients on observe 12 covariables :

-   L'âge du patient (quantitative ordinale)

-   La présence d'une anémie (qualitative)

-   La creatine phosphokynase : enzyme fabriquée notamment dans le coeur qui peut indiquer un signe de maladie du coeur (quantitative) [@creat_phosphokinase]

-   La présence de diabète (qualitative)

-   La fraction d'ejection (quantitative ordinale) c'est la fraction de sang que le coeur peut ejecter en une contraction (+ c'est élevé, + c'est bien)[@heart_failure_matters_fraction_ejection]

-   La présence d'une pression sanguine élevée (qualitative)

-   Les plaquettes : quantité de plaquette par mm\^3 (quantitative) (la normale est entre 150 000 et 300000) [@cancer_societe_thrombocytopenie]

-   Creatinine : déchêt chimique produit par les muscle une valeur élevée indique une déterioration de la fonction de certains muscle (rénale, cardiaque,...) (quantitative)[@creatinine_irr]

-   le sodium (quantitative)

-   le sexe du patient (qualitative)

-   time (quantitative) : temps avant d'observer l'évènement

```{r, imports, warning=FALSE, echo=FALSE, message=FALSE}
library(survival)
library(corrplot)
library(MASS)
library(latex2exp)
library("dplyr")
library(kableExtra)
library("fitdistrplus")
library(ggplot2)
library(rpart)
library(rpart.plot)
library(pROC)
library(ggplot2)
library(dplyr)
library(gridExtra)

rm(list=ls())
setwd("C:/Users/yoyoc/Desktop/5A/ADS")
```

Avant toute chose, il faut effectuer une série de prétraitement sur les données. Notamment rendre les variables "binaires" interprétable de façon qualitative par R.

```{r}
# Ouverture du df
heart_df <- read.csv("heart_failure_clinical_records_dataset.csv")

# Visualisation du df
#View(heart_df)
head(heart_df)

# Quelques valeurs de base
names(heart_df) # nom des variables
str(heart_df)
summary(heart_df)
#attach(heart_df)

# Prétraitement
heart_df$sex = as.factor(heart_df$sex)
heart_df$high_blood_pressure = as.factor(heart_df$high_blood_pressure)
heart_df$anaemia = as.factor(heart_df$anaemia)
heart_df$diabetes = as.factor(heart_df$diabetes)
heart_df$smoking = as.factor(heart_df$smoking)

str(heart_df)

heart_df_num = select_if(heart_df, is.numeric)
heart_df_covar = heart_df[,1:11]
```

Les pré-traitement étant terminés, on peut maintenant apporter des éléments d'ordre descriptifs afin de comprendre les répartitions des individus dans le jeu de données.

```{r}
# Corrélations éventuelles ?
corrplot(cor(heart_df_num))

# Quantité de données censurées 
hist(heart_df$DEATH_EVENT)
```

```{r warning=FALSE}
numeric_vars <- names(heart_df)[sapply(heart_df, is.numeric)]

plot_list <- lapply(numeric_vars, function(var) {
  ggplot(heart_df, aes_string(x = var)) +
    geom_histogram(fill = "skyblue", color = "black", bins = 30) +
    theme_minimal() +
    labs(title = paste("Distribution de", var), x = var, y = "Fréquence")
})

do.call(grid.arrange, c(plot_list, ncol = 2))

```

```{r warning=FALSE}
ggplot(heart_df, aes(x = age, fill = DEATH_EVENT)) +
  geom_histogram(bins = 20, alpha = 0.7, position = "identity") +
  theme_minimal() +
  labs(title = "Distribution de l'âge par statut de décès",
       x = "Âge", y = "Fréquence", fill = "Décès") +
  scale_fill_manual(values = c("0" = "skyblue", "1" = "red"))

```

# Prérequis mathématiques

On aimerait, à partir des données présentées, estimer les chances de survie d'un individu après une durée $t$. Autrement dit, on souhaite estimer la probabilité que l'évènement (ici, le décès) survienne après un certain temps $t$. Définir ce concept necessite un rappel sur quelques fonctions d'analyses statistiques de bases.

## Censure

Dans une étude où l'on suit les patients au cours du temps ("follow-up study" en anglais), les patients rentrent à des instants différents et l'évènement d'intérêt se produit à des instants différents. On identifie à partir de là **trois grandeurs** pour un individu $i=1,\dots,n$ : [@cours_ads_philippe_saint_pierre]

-   Son temps de survie, temps de l'évènement : $Y_i$

-   Son temps de censure $C_i$

-   La durée réellement observée $T_i = min(Y_i,C_i)$

-   L'observation $t_i$ pour le sujet $i$ de $T_i$

-   Le statut de l'observation associé à la durée réellement observée $\delta_i$ ( $=1$ si l'évènement est observé )

### Moment de la censure

La qualification de la censure dépend du moment où l'évènement est ou n'est pas observé :

-   **Censure à droite** : Chez certains patients, on n'observe pas le moment où se produit l'évènement à cause d'une censure à droite.\
    [*Exemple*]{.underline} : *Le patient abandonne l'étude (quitter spontanément, indisponibilité, déménagement, etc.). L'étude s'achève avant que l'évènement ne se produise ([censure administrative]{.underline})*

-   **Censure à gauche** : Il s’agit du cas où la date d’origine n’est pas observée de telle sorte que la durée de vie n’est pas connue, que l’évènement d’intérêt se soit produit ou non. On ne connait pas toujours la date d'entrée dans l'étude.\
    [*Exemple*]{.underline} *: étude du moment de descente des arbres des babouins de la réserve d'Amboli, Kenya. Si les biologistes arrivent alors qu'il est déjà descendu, on observe alors le maximum entre l'heure de descente et l'heure d'arrivée des observateurs.*

-   **Censure par intervalle** : la date de changement d’état n’est pas renseignée, mais un intervalle de temps est connu. Ce phénomène est caractéristique des enquêtes où le suivi est réalisé avec des rendez-vous réguliers. La seule information disponible sur la durée de survie est caractérisée par les dates des rendez-vous entre lesquelles l’évènement d’intérêt s’est produit\
    [*Exemple*]{.underline} *: suivi des patients ayant un plombage. l'évènement observé est la fracture du plombage ou une complication endodontique. L'examen ne peut se faire que chez le dentiste. Cette observation est donc censurée par intervalle. Certains patients gardent leurs couronnes toutes leur vie donc ces données seront censurées à droite si pas d'évènement.*

### Type de censure

On distingue les censure par la date d'origine de l'évènement, on distingue aussi -et surtout- les censures par leur [**mécanisme générateur**]{.underline}.

1.  **La censure de type I** (fixée)\
    C'est la [censure administrative]{.underline}\
    Soit $C$ une valeur fixée. Au lieu d'observer la variable $Y_i$ sur toute la durée, on n'observe uniquement $Y_i\leq C$. On utilise la notation $T_i = Y_i \wedge C = min(Y_i,C)$ (cas à droite) . Dans ce cas, la durée n'est pas observable au-delà (ou avant) d'une durée maximale fixée à l'avance et est identique à tous les individus.\
    [*Exemple*]{.underline} *: on étudie sur 5 ans l'apparition de cancers. Au bout de 5 ans, certains patients n'ont pas développé de cancers : ces données sont censurées de type I (fin de l'étude)*

2.  **La censure de type II** (attente)\
    Le protocole de collecte suppose d'observer les durées de vie de $n$ individus jusqu'à ce que $R$ individus aient vu l'évènement d'intérêt se produire.\
    [*Exemple*]{.underline} *: les données* $Y_1\leq \dots \leq Y_R$ sont récupérées, toutes les observations entre $R$ et $n$ sont censurées de type II

3.  **La censure de type III** (aléatoire)\
    Il s'agit d'une information incomplète liée à un [évènement non fixé]{.underline} par le protocole de suivi. Cette censure arrive lors de la sortie de l'étude d'un sujet avant la fin de la période de suivi fixée par le protocole. *(exemple à droite)\
    *À droite, les données disponibles sont les suivantes : $T_i = Y_i \wedge C_i$, $\mathbb{1}_{\delta_i}$\
    [*Exemple*]{.underline} *: perte de vue, arrêt ou changement de traitement*

## Fonction de densité

La fonction de densité de probabilité $f$ représente la limite de probabilité que l'évènement se produise "au temps $t$". Avec $Y$, la v.a continue positive qui représente le temps de survie d'un individu par exemple.

$$ f(t) = lim_{\Delta t\rightarrow 0} \frac{\mathbb{P}(t<Y\leq t+\Delta t)}{\Delta t} $$

[*Exemple*]{.underline} *: Soit* $t\geq 0$ *, le temps et* $\mu \geq 0$ *la durée moyenne de survie d'un individu. Un modèle simple régulièrement utilisé est le modèle de densité exponentielle :* $f(t)=\frac{1}{\mu}e^{-\frac{1}{\mu}t}$

::: {style="color:red;"}
D'un point de vue du biologiste, la fonction $f$ de densité correspond à la proportion de décès entre $t$ et $\Delta t$ rapporté au nombre total d'individus à l'instant initial $t_0$.
:::

## Fonction de répartition

$F$ est la fonction de répartition associée à la fonction de densité $f$, c'est la probabilité que l'évènement se produise **avant** $t$.$$ \begin{align*} F(t) &= \mathbb{P}(Y\leq t) = 1-S(t) = \int_0^tf(x)dx\\ \end{align*} $$

Valeurs remarquables : $F(0)=0$ , $F(\infty)=1$

[Rappel]{.underline} : pour une variable aléatoire continue positive comme le temps ou la durée de survie $T$, la fonction $F$ de répartition correspond à l'air sous la courbe de la fonction de densité. $F$ est strictement croissante.

[*Exemple (suite)*]{.underline} *:* $F(t) =\int_0^t\frac{1}{\mu}e^{-\frac{1}{\mu}x}dx \Leftrightarrow F(t) = 1-e^{-\frac{1}{\mu}t}$

[Complément]{.underline} : la définition de la fonction de réparition apporte naturellement une nouvelle façon d'interpréter la fonction de densité. En effet : $f(t)=\frac{dF(t)}{dt}$ représente la probabilité que le risque se produise exactement au temps $t$.

## Fonction de survie

Soit $S$ la fonction de survie, c'est la probabilité que l'évènement se produise **après** $t$ :

$$ \begin{align*} S(t) &= \mathbb{P}(Y>t) = 1- \mathbb{P}(Y<t) = 1 - F(t) = \int_t^\infty f(x)dx\\ \end{align*} $$

Valeurs remarquables : $S(0)=1$, $S(\infty)=0$

[Remarque]{.underline} : $S$ est strictement décroissante

[*Exemple (suite)*]{.underline} *:* $S(t) = 1-F(t) = 1-\left(1-e^{-\frac{1}{\mu}t}\right)\Leftrightarrow S(t) = e^{-\frac{1}{\mu}t}$

Concrètement, dans le contexte de l'étude la fonction de survie, c'est la probabilité qu'un décès soit observé après un temps $t$.

# Estimation de la fonction de survie

En pratique, on estime la fonction de survie $S$ à partir des données. La formule de **Kaplan-Meier** pour le calcul de l'estimation de la fonction de survie $S$ :

$n_i$ étant le nombre d'observations restantes non censurées (= nb de survivants) juste avant $t_i$. Et $d_i$ est le nombre d'évènements (=nb de décès) observés à l'instant $t_i$.

Produit des probabilités de survie à chaque étape précédente, conditionnellement à la survie jusqu'à cette étape. Prendre en compte les probabilités de survie des étapes précédentes.

$$ \begin{align*} \hat{S}(t_j)&=\frac{n_j-d_j}{n_j} \hat{S}(t_{j-1}) &\text{Proportion de survivants en $t_j$} \\ \hat{S}(t_j)&=\frac{n_j-d_j}{n_j} \prod_{i=1}^{j-1}\frac{n_i-d_i}{n_i}\\ \hat{S}(t_j)&=\prod_{i=1}^j\frac{n_i-d_i}{n_i} = \prod_{i=1}^j \mathbb{P}(T>t_j|T\geq t_j) \end{align*} $$

## Estimateur de la variance

L'estimateur de **Greenwood** permet d'estimer la variance de la fonction de survie $S$.

$$ \hat{\sigma}\left(\hat{S}(t_j)\right) = \hat{S}(t_j)^2\sum_{i=1}^j\frac{d_i}{n_i(n_i-d_i)} $$

## Intervalle de confiance

$$
IC_{95\%}\left(S(t_j)\right) = \hat{S}(t_j) \pm 1.96\times\hat{\sigma}\left(\hat{S}(t_j)\right)
$$

```{r}
Y = Surv(heart_df$time, heart_df$DEATH_EVENT)
fit1 <- survfit(Y~1,conf.type="plain", type=c("kaplan-meier"))
#summary(fit1)

plot(fit1,
     col="blue",
     lwd=2,
     main='Estimation de Kaplan-Meier avec censure',
     xlab = "t",
     ylab = "S")

grid(nx = NULL, ny = NA, lty = 3, col = "gray", lwd = 1)
```

Faisons quelques observations :

1.  Comme prévu dans sa définition, on a une ordonnée à l'origine à 1. Cela, traduit que la survie au moment où l'étude commence est maximale (la cohorte ne contient que des patients vivants !)
2.  La survie à 50 jours est d'environ 78%. Autrement dit, le probabilité que l'évènement d'intérêt (i.e. le décès) soit observé après le cinquentième jours est de 78%.
3.  "Asymptotiquement", on observe une survie à 58%.

On veut vérifier qu'il y a bien un intérêt a utiliser des données censurée. On observe ci-dessous les données de survie lorsqu'il n'y a pas de censure. Une méthode possible est de supposer que tous les évènements sont survenus. Dans ce cas, on s'attend logiquement à une courbe de survie qui tombe à 0 au au plus le dernier jour.

```{r}
# Modèle sans censure
Y_no_cens <- Surv(heart_df$time, rep(1, nrow(heart_df)))  # Ici, on considère que tous les événements sont survenus
fit_no_cens <- survfit(Y_no_cens ~ 1, conf.type="plain", type="kaplan-meier")
#summary(fit_no_cens)
plot(fit_no_cens,
     col="red",
     lwd=2,
     main='Estimation de Kaplan-Meier sans censure',
     xlab = "t",
     ylab = "S")
```

L'hypothèse que nous avions prévu avant d'afficher l'estimateur de la fonction de survie sans la présence de censurese vérifie. En comparaison avec la survie censurée, on peut une fois de plus poser plusieurs hypothèses :

-   Soit les courbes se confondent, ce qui laisserait penser que la censure est décorrélée de la mortalité

-   Soit les courbes se confondent sur une période et se sépare

-   Soit les deux courbes ne se confondent en aucun point

```{r}
# Comparaison des courbes avec et sans censure
plot(fit1, col="blue", lwd=2, main='Estimation de Kaplan-Meier, avec et sans censure',
     pch=11,
     xlab="t",
     ylab="S")

lines(fit_no_cens, col="red", lwd=2)
grid(nx = NULL, ny = NA, lty = 3, col = "gray", lwd = 1)
legend("topright", legend=c("Avec censure", "Sans censure"), col=c("blue", "red"), lwd=2)
```

Comme on le voit la deuxième hypothèse semble ici s'observée. Jusqu'au 70 e jour environ, les courbes estiment de la même façon la survie. Après ce jour, une séparation de l'estimation. Cela s'explique assez simplement. En effet sur la courbe avec censure, c'est à peu près à ce jour là qu'aparaît des plateaux, on censure alors rarement pour cause d'un évènement observé. De fait, comme la courbe sans censure suppose que tous les individus observeront l'évènement, alors l'estimation change complètement. Finalement il y a 57% de différences entre les deux courbes.

La courbe avec censure donne une estimation plus juste et plus réaliste des probabilités de survie, car elle considère que certains patients n'ont pas encore subi l'événement (décès) à la fin de la période d'observation. Elle montre la probabilité de survie réelle, en tenant compte des patients toujours en vie mais non suivis jusqu'au bout. La courbe sans censure représente un scénario extrême où tous les patients finissent par subir l'événement, ce qui est généralement irréaliste.

# Survie avec covariable: un exemple

Dans cet exemple il sera question de faire un exemple d'analyse de survie avec une covariable. On cherche à comparer la survie des patients de la cohorte relativement à une seule covariable. La covariable dans cet exemple sera la variable binaire de présence de pression artérielle *(HBP en anglais)*.

```{r}
# Second Kaplan Meier (avec filtre)
t1 <- heart_df$time[heart_df$high_blood_pressure==1]
e1 <- heart_df$DEATH_EVENT[heart_df$high_blood_pressure==1]
Y1 <- Surv(t1,e1)

t2 <- heart_df$time[heart_df$high_blood_pressure==0]
e2 <- heart_df$DEATH_EVENT[heart_df$high_blood_pressure==0]
Y2 <- Surv(t2,e2)

plot(survfit(Y1~1), col="blue", main="Estimation Kaplan de la fonction de survie des patients atteint d'une\n insuffisance cardiaque qui ont, ou non, de l'hyptertension")
lines(survfit(Y2~1),col="red")
legend(0,0.2,legend=c("hypertension", "pas d'hypertension"), col=c("blue", "red"),
       lty=c(1,1), cex = 0.7)
```

On remarque qu'en moyenne, les patients atteints de de pression artérielle survivent moins bien. Mais on voyons aussi que les intervalles de confiances se chevauchent souvent. On se demande alors si les deux survie sont bien significativement différentes.

On fait alors deux tests pour vérifier ou infirmer notre conjecture précédente :

On effectue deux tests afin de croiser des résultats, tout d'abord avec le test du log-rank :

$$H_0 : S_A(t) \approx S_B(t)$$

-   **Test du log rank** [@alberti-2005]: Sa statistiques de test est donnée par :\
    $$\chi^2 = \frac{(O_A-E_A)^2}{E_A} + \frac{(O_B-E_B)^2}{E_B}$$\
    où sur les deux groupes $A$ et $B$, on note les décès observés $O$, le nombre de décès attendu $E_{X_i}$

-   **Test de Gehan Wilcoxon :\
    **La statistique est basée sur les rangs pondérés des temps de survie. Les contributions des décès observés $O$ et des décès attendus $E$ sont pondérées pour privilégier les événements précoces\
    $$U = \sum _{i\in A}W_i- \sum _{j\in B}W_j$$\
    \$W_i\$ : poids attribué à chaque individu, généralement égal au nombre de personnes encore en risque (i.e., non censurées et vivantes) au moment de l'événement,

    Les décès observés sont pris en compte avec un poids, et les décès attendus sont calculés de manière similaire mais en tenant compte des poids.

    -   Ce test attribue un poids plus élevé aux événements survenant **tôt dans le suivi**.

    -   Il est donc plus sensible aux différences précoces entre les courbes de survie.

```{r}
survdiff(Y~high_blood_pressure,data=heart_df,rho=0) #Test du log-rank
survdiff(Y~high_blood_pressure,data=heart_df,rho=1) #Gehan-wilcoxon
```

Les deux p-value étant inférieurs à 0.05, on peut raisonnablement affirmer qu'il y a suffisamment d'éléments pour rejeter l'hypothèse d'égalité des deux fonctions de survie. Autrement dit, la survie d'un patient atteint d'insuffiance cardiaque diffère significativement qu'il ait, ou non, de l'hypertension.

# Estimation de la fonction de risque

Soit $h$ la fonction de risque :$$ h(t) = lim_{\Delta t\rightarrow 0} \frac{\mathbb{P}(t<T\leq t+\Delta t | T>t)}{\Delta t} $$

[*Exemple (suite)*]{.underline} *:* $h(t)=\frac{f(t)}{S(t)}=\frac{\frac{1}{\mu}e^{-\frac{1}{\mu}t}}{e^{\frac{1}{\mu}t}}=\frac{1}{\mu}$

::: {style="color:red;"}
D'un point de vue du biologiste, la fonction $h$ de risque correspond au taux de mortalité instantané entre $t$ et $\Delta t$ sachant que le temps de survie $T$ est supérieur à $t$.
:::

On sait que la fonction $h$ de risque admet comme égalité :

$$ \begin{align*} h(t) &= \frac{f(t)}{S(t)} = \frac{\frac{dF(t)}{dt}}{S(t)} = \frac{\frac{d(1-S(t))}{dt}}{S(t)} = - \frac{1}{S(t)}\frac{dS(t)}{dt}\\ &\Leftrightarrow h(t)dt = - \frac{dS(t)}{S(t)} \\ &\Leftrightarrow \int_0^th(x)dx=-\int_0^t \frac{dS(x)}{S(x)} dx\\ &\Leftrightarrow H(t) = - [ ln(S(t))-ln(S(0))] &\text{$H$ est la fonction de risque cumulé}\\ &\Leftrightarrow \color{red}{H(t) = -ln(S(t))}\\ &\Leftrightarrow \color{red}{S(t) = e^{-\int_0^th(x)dx}} \end{align*}\\ $$Ces différentes équivalences permettent de définir quelques grandeurs utiles.

Le **risque cumulé** est défini comme la somme continue des risques instantanés. D'où :

$$
H(t) = \int^t_0h(x)dx
$$

Le risque cumulé peut être estimé à partir de de la fonction de la fonction de survie. On est alors bien tenté de d'estimer cette grandeur à partir de l'estimateur de Kaplan-Meier. Cette méthode est bien connu, c'est l'**estimateur de Breslow** défini ci-dessous :

$$\hat{H}(t)=-\log(S(t))$$ On continue de justifier que la censure est necessaire en comparant les estimations de risques cumulé et instantané avec et sans censure.

```{r}
# Estimation de la fonction de risque cumulé avec Breslow
H <- -log(fit1$surv)
H_without_censor <- -log(fit_no_cens$surv)

# Tracé des deux courbes avec des paliers
plot(H, col = 'blue',  type ="s", lwd =1,
     main = TeX(r'($\hat{H}_{Breslow}$ avec et sans censure)'),
     xlab = TeX(r'($t$)'), 
     ylab = TeX(r'($\hat{H}(t)$)')
)

# Ajouter ligne sans censure
lines(H_without_censor, col = 'red', lwd = 1, lty = 1)  # Ligne pour sans censures

# Ajouter une légende
legend("bottomright", legend = c("Avec censures", "Sans censures"),
       col = c("blue", "red"), lwd = 1, lty = c(1, 1))

```

Il existe également un autre estimateur, celui de **Nelson-Aalen**:

$$
\hat{H}(t)=\sum_{t_i\leq t} \frac{n_i}{\delta_i}
$$

où :

-   $t_i$​ sont les temps d'événements observés,

-   $\delta_i$ est un indicateur indiquant si un événement s'est produit (1 si l'événement s'est produit, 0 sinon),

-   $n_i$ est le nombre de sujets à risque juste avant $t_i$​.

```{r}
naest <- cumsum(fit1$n.event/fit1$n.risk)

naest_no_censor <- cumsum(fit_no_cens$n.event/fit_no_cens$n.risk)


# Tracé des deux courbes
plot(fit1$time, naest, type = "s", pch = 1, col = 'blue',
     main = TeX(r'($\hat{H}_{Nelson-Aalen}$ avec et sans censure)'),
     xlab = TeX(r'($t$)'),  
     ylab = TeX(r'($\hat{H}(t)$)')
)

# Ajouter les points et lignes pour sans censures
lines(fit_no_cens$time, naest_no_censor, pch = 2, col = 'red') 

# Ajouter une légende
legend("bottomright", legend = c("Avec censures", "Sans censures"),
       col = c("blue", "red"), lty = c(1, 1))
```

```{r}
plot(fit1$time, -log(fit1$surv), type = "s", pch = 1, col = 'blue',
     main = TeX(r'($\hat{H}$ avec Breslow et Nelson-Aalen, avec et sans censure)'),
     xlab = TeX(r'($t$)'),  
     ylab = TeX(r'($\hat{H}(t)$)')
)

# Ajouter les lignes et points pour Breslow sans censures
lines(fit_no_cens$time, -log(fit_no_cens$surv), pch = 2, col = 'red')

# Ajouter les lignes et points pour Nelson-Aalen avec censures
lines(fit1$time, naest, pch = 3, col = 'green')

# Ajouter les lignes et points pour Nelson-Aalen sans censures
lines(fit_no_cens$time, naest_no_censor, pch = 4, col = 'purple')

# Ajouter une légende pour distinguer les courbes
legend("bottomright", legend = c("Breslow avec censures", "Breslow sans censures", 
                              "Nelson-Aalen avec censures", "Nelson-Aalen sans censures"),
       col = c("blue", "red", "green", "purple"),
       lty = c(1, 1, 1, 1))
```

On estime également le risque instantané.

```{r}
naest <- fit1$n.event/fit1$n.risk

naest_no_censor <- fit_no_cens$n.event/fit_no_cens$n.risk


# Tracé des deux courbes
plot(fit1$time, naest, type = "s", pch = 1, col = 'blue',
     main = TeX(r'($\hat{h}_{Nelson-Aalen}$ avec et sans censure)'),
     xlab = TeX(r'($t$)'),  
     ylab = TeX(r'($\hat{h}(t)$)')
)

# Ajouter les points et lignes pour sans censures
lines(fit_no_cens$time, naest_no_censor, pch = 2, col = 'red') 

# Ajouter une légende
legend("bottomright", legend = c("Avec censures", "Sans censures"),
       col = c("blue", "red"), lty = c(1, 1))
```

Il est normal d'observer des pics. En effet, comme la fonction de risque cumulée est une fonction dite "en escalier", alors la fonction de risque instantanée est nulle "la plupart" du temps et lorsqu'un évènement survient le risque instantannée augmente ponctuellement.

# Tests de conformité

On cherche à présent à trouver une densité standard permettant d'approcher au mieux la fonction la fonction de survie. Pour chacune des distributions strandards, on affichera quatre graphiques permettant un diagnostic visuel.

-   En haut à gauche : on a la réparition des données sous une forme empirique et sous une forme théorique, qui est la meilleure approximation par cette distribution. Plus la distribution théorique est proche de celle empirique, plus la distribution approche correctement la survie.

-   En haut à droite : le Quantile-Quantile plot, qui permet de comparer les quantile théorique et empirique.

-   En bas à gauche : le fonction de répartition, qui comme la densité théorique et empirique : permet vérifier si une distribution suit une loi particulière (comme une loi normale).

-   En bas à droite : le Probability-Probability plot. Une ligne droite indique que les distributions ont des probabilités similaires.

## Loi Gamma

La loi gamma est la distribution continue définie par les paramètres $\alpha > 0$ ( forme) et $\beta > 0$ (échelle). La fonction de densité est donnée par :

$$
f(x; \alpha, \beta) =
\begin{cases}
\frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}, & \text{si } x > 0, \\
0, & \text{si } x \leq 0,
\end{cases}
$$

où $\Gamma(\alpha)$ est la fonction gamma définie par :

$$
\Gamma(\alpha) = \int_0^\infty t^{\alpha-1} e^{-t} \, dt.
$$

La moyenne de la loi gamma est donnée par $\mu = \frac{\alpha}{\beta}$ et sa variance par $\sigma^2 = \frac{\alpha}{\beta^2}$.

```{r}
g <- fitdist(fit1$surv,"gamma")
plot(g, demp=TRUE,histo = TRUE)
```

## Loi de Weibull

La loi de Weibull est une distribution continue définie par les paramètres $\lambda > 0$ (échelle) et $k > 0$ (forme). La fonction de densité est donnée par :

$$
f(x; \lambda, k) =
\begin{cases}
\frac{k}{\lambda} \left( \frac{x}{\lambda} \right)^{k-1} e^{-\left( \frac{x}{\lambda} \right)^k}, & \text{si } x \geq 0, \\
0, & \text{si } x < 0.
\end{cases}
$$

La fonction de répartition est donnée par :

$$
F(x; \lambda, k) =
\begin{cases}
1 - e^{-\left( \frac{x}{\lambda} \right)^k}, & \text{si } x \geq 0, \\
0, & \text{si } x < 0.
\end{cases}
$$

D'espérence $\mathbb{E}[X] = \lambda \Gamma\left(1 + \frac{1}{k}\right)$ et de variance$\text{Var}(X) = \lambda^2 \left[ \Gamma\left(1 + \frac{2} {k}\right) - \Gamma^2\left(1 + \frac{1}{k}\right) \right]$

```{r}
w <- fitdist(fit1$surv,'weibull')

summary(w)
plot(w, demp=TRUE, histo = TRUE)
```

## Loi beta

La loi Beta est une distribution continue définie sur l'intervalle $[0, 1]$, avec deux paramètres $\alpha > 0$ et $\beta > 0$. La fonction de densité est donnée par :

$$
f(x; \alpha, \beta) =
\begin{cases}
\frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha, \beta)}, & \text{si } 0 \leq x \leq 1, \\
0, & \text{sinon},
\end{cases}
$$

où $B(\alpha, \beta)$ est la fonction Beta, définie par :

$$
B(\alpha, \beta) = \int_0^1 t^{\alpha-1} (1-t)^{\beta-1} \, dt = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)},
$$

D'espérence : $\mathbb{E}[X] = \frac{\alpha}{\alpha + \beta}$, et de variance $\text{Var}(X) = \frac{\alpha\beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$.

```{r}
b <- fitdist(fit1$surv,"beta")

summary(b)
plot(b, demp=TRUE, histo = TRUE)
```

## Loi exponentielle

La loi exponentielle est une distribution continue définie par un paramètre $\lambda > 0$, qui représente le taux d'événements. La fonction de densité donnée par :

$$
f(x; \lambda) =
\begin{cases}
\lambda e^{-\lambda x}, & \text{si } x \geq 0, \\
0, & \text{si } x < 0.
\end{cases}
$$

La fonction de répartition est donnée par :

$$
F(x; \lambda) =
\begin{cases}
1 - e^{-\lambda x}, & \text{si } x \geq 0, \\
0, & \text{si } x < 0.
\end{cases}
$$

D'espérence $\mathbb{E}[X] = \frac{1}{\lambda}$ et de variance $\text{Var}(X) = \frac{1}{\lambda^2}$.

```{r}
e <- fitdist(fit1$surv,"exp")

summary(e)
plot(e, demp=TRUE, histo = TRUE)
```

## Loi normale

La loi normale est une distribution continue définie sur $\mathbb{R}$, avec deux paramètres $\mu \in \mathbb{R}$ (moyenne) et $\sigma >0$ (l'écart type). La fonction de densité est donnée par:

$$
\mathcal{N}(x; \mu, \sigma) =
\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

D'espérence $\mu$ et de variance $\sigma ^2$ . Densité symétrique autour de $\mu$.

```{r}
n <- fitdist(fit1$surv,"norm")

summary(n)
plot(n, demp=TRUE, histo = TRUE)
```

## Loi lognormale

La loi lognormale est une distribution continue définie par

$$
f(x; \mu, \sigma) =
\begin{cases}
\frac{1}{x \sqrt{2\pi\sigma^2}} \exp\left(-\frac{(\ln(x)-\mu)^2}{2\sigma^2}\right) & \text{si } x > 0 \\
0, & \text{sinon}.
\end{cases}
$$

D'espérence $\mathbb{E}[X] = \exp\left(\mu + \frac{\sigma^2}{2}\right)$ et de variance $\text{Var}(X) = \left(\exp(\sigma^2) - 1\right) \exp\left(2\mu + \sigma^2\right).$

```{r}
ln <- fitdist(fit1$surv,"lnorm")

summary(ln)
plot(ln, demp=TRUE, histo = TRUE)
```

## Synthèse

```{r}
liste = list(ln, w,b,n,g)
cdfcomp(liste, legendtext=c("lognormal", "weibull", "beta", "normal", "gamma"))
denscomp(liste, legendtext=c("lognormal", "weibull", "beta","normal", "gamma"))
qqcomp(liste, legendtext=c("lognormal", "weibull", "beta","normal", "gamma"))
ppcomp(liste, legendtext=c("lognormal", "weibull", "beta","normal", "gamma"))

synthese <- gofstat(liste)

```

On test donc la conformité des lois précédentes avec l'estimation de la fonction de survie avec différents tests. En pratique, l'hypothèse a testé est toujours la même ! En revanche, c'est bien la statistique de test, i.e. la façon dont va être calculé la distance entre les fonctions de répartitions. [@darling-1957]

$$
H_0: \text{"La fonction de survie a pour fonction de réparition}\ F_0 "
$$

**Kolmogorov Smirnov :** Derrière ce test, on prend l'écart maximale entre la fonction de répartition entre les deux lois:

$$
\sup_x |F_n(x)-F(x)|
$$

Pour les deux prochains tests, on utilisera la distance suivante :

$$
n\int_{-\infty}^{+\infty} \left(F_n(x)-F(x)\right)^2w(x)dF(x)
$$

où $n$ est le nombre d'observations, et $w(x)$, une fonction de poids. Seul le poid sera changé

**Test de Anderson-Darling :** derrière ce test, on prend une fonction de poid égale à $[F(x)(1-F(x))]^{-1}$ . Cela permet de mettre plus de poids sur les observations sur les queues de distributions.

$$ n\int_{-\infty}^{+\infty} \frac{\left(F_n(x)-F(x)\right)}{F(x)(1-F(x))} ^2dF(x) $$

**Test de CVM :** Derrière ce test, on prend une fonction de poid constante égale à 1:

$$
n\int_{-\infty}^{+\infty} \left(F_n(x)-F(x)\right)^2dF(x)
$$

On réalise également un test du $\chi ^2$ définit par :

$$
H_0: \text{"les deux échantillons proviennent de deux variables aléatoires suivant la même loi}"
$$

On utilise deux métriques dans la suite de la présentation pour évaluer des modèles que sont l AIC et BIC. On les définit ci-dessous [@schwarz-1978] : $$ AIC = 2\times k-2\log(\hat{L}) $$

où $k$ est le nombre de paramètres estimés dans le modèle et $\hat{L}$ la log vraissemblance du modèle.

Le BIC est :

$$
BIC = \log(N) \times k-2\log(\hat{L})
$$

où $N$ est le nombre d'observations du jeux de données. Comme on le vois, la pénalité dépend de la taille de l'échantilllon et pas seulement du nombre de paramètre.

Dans les deux cas, le modèle sélectionné sera toujours **celui avec le plus petit score**.

```{r}
gof_df <- data.frame(
  Distribution = c("lognormal", "weibull", "beta","normal", "gamma"), 
  AIC = synthese$aic,                   # Critère AIC
  BIC = synthese$bic,                   # Critère BIC
  AD = synthese$ad,
  KS = synthese$ks,
  CVM = synthese$cvm,
  chisq = synthese$chisq,               # Test du chi-carré
  chisq_pvalue = synthese$chisqpvalue   # Valeur p du chi-carré
)

gof_df <- gof_df %>%
  mutate(
    AIC = round(AIC, 1),
    BIC = round(BIC, 1),
    AD = round(AD, 2),
    KS = round(KS, 2),
    CVM = round(CVM, 2),
    chisq = round(chisq, 2),
    chisq_pvalue = round(chisq_pvalue, 5)
  )



gof_df %>%
  kable(
    format = "html",
    caption = "Goodness-of-Fit Table: Résumé des critères pour chaque distribution",
    col.names = c(
      "Distribution", "AIC", "BIC", "Anderson-Darling (AD)",
      "Kolmogorov-Smirnov (KS)", "CVM", "Chi-squared", "P-value"
    ),
    align = c("l", "c", "c","c", "c", "c", "c", "c") 
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#0073C2") %>% 
  column_spec(1, bold = TRUE, color = "black") 
```

Chi-squared : toutes les p values sont \< 0.05 donc on rejette H0 \<=\> les échantillons proviennent de deux lois différentes. Beta, weibull : on est sûr. Normal : moins sûr. log normal et gamma : pas sûr du tout (on ne peux pas rejetter H0).

## Nuances dans les résultats

```{r}
print(synthese$kstest)

print(synthese$cvmtest)

print(synthese$adtest)
```

<https://github.com/cran/fitdistrplus/blob/209b492bd5642387b960bf62b4219e35f7833d47/R/gofstat.R#L284>

For data sets with more than 5 observations and for distributions for which the test is described by Stephens (1986) for maximum likelihood estimations (`"exp"`, `"cauchy"`, `"gamma"` and `"weibull"`), the Cramer-von Mises and Anderson-darling tests are performed as described by Stephens (1986). Those tests take into account the fact that the parameters are not known but estimated from the data by maximum likelihood. The result is the decision to reject or not the distribution at the significance level 0.05. Those tests are available only for maximum likelihood estimations.

not computed pour cette raison

```{r}
adtest = 0
synthese <- gofstat(ln)

ad = synthese$ad
distname = "lnorm"
n = ln$n

if ((distname == "norm" | distname == "lnorm") & n>=5) {
  a2mod <- ad*(1+0.75/n+2.25/n^2)
  adtest <- ifelse(a2mod>0.752,"rejected","not rejected")
} 

print(adtest)
```

```{r}
distname = "lnorm"
cvmtest = 0

cvm = synthese$cvm
distname = "lnorm"
n = ln$n


if ((distname == "norm" | distname == "lnorm") & n>=5) {
  w2mod <- cvm*(1+0.5/n)
  cvmtest <- ifelse(w2mod>0.126,"rejected","not rejected")
} 

print(cvmtest)
```

# Étude en présence de covariable

## Motivation d'un nouveau modèle

Prenons un exmple afin de motiver l'étude spécifique de la survie :

On se pose la question : Est-ce que le tabagisme réduit le temps d'apparition d'un décès ? Afin de répondre à cette question, on peut avoir l'idée de réaliser une régression linéaire, dont on rappelle la définition :

$$ \mathbb{E}[Y|x_1,x_2,\dots]=\beta_0+\beta_1x_1+\beta_2x_2+\dots $$

où :

-   $Y$ : variable aléatoire de temps avant l'évènement

-   $x_1,x_2,\dots$ : variables explicatives d'intérêt

-   $\beta_1,\beta_2,\dots$ : coefficients de régression

Problème : Dans ce cas on ne fait pas de différence entre $T$ et $Y$ ! On observe $T$ mais pas $Y$ ! Aussi, nous n'expliquons pas la variable d'évènement $\delta$.

On essaie alors de faire une régression logistique, dont on rappelle la définition :

$$ logit\left(\mathbb{P}(\delta=1|x_1,x_2,\dots) \right) =\beta_0+\beta_1x_1+\beta_2x_2+\dots  $$

où :

-   $\delta$ : variable aléatoire binaire à expliquer

-   $x_1,x_2,\dots$ : variables explicatives d'intérêt

-   $\beta_1,\beta_2,\dots$ : coefficients de régression

On rappelle que $exp(\beta_1)$ représente l'ODD ratio du groupe $x_1=1$ par rapport au groupe $x_1=0$.

Problème: une étude s'étalant dans la durée a plus de chance de donner $\delta=1$ . De plus, il y a un biais dans l'estimation des effets des variables explicatives qui sont également explicative du temps de suivi : on perd une information !

Il y a donc besoin de créer un nouveau modèles.

## Ecriture du modèle de Cox

Lorsque la variable $X_i$ est catégorielle

La forme générale continu du modèle de **Cox** est la suivante :

$$ h(t,X_i) = h_0(t)e^{\beta_1X_1+\dots+\beta_kX_k} $$

Où $h$ est le risque, $h_0$ est le risque de base, $t$ le temps, $X_i$ les variables explicatives ou prédictives et $\beta_i$ les coefficients de la régression (risque instantané relatif, taux relatif). On remarque immédiatement que c'est un modèle dit log-linéaire :

$$
\log(h(t,X_i)) = \log(h_0(t)) + \beta_1X_1+\dots+\beta_kX_k
$$

On pose en général $\eta =\beta_1X_1+\dots+\beta_kX_k$ *(rmq : l'ordonnée à l'origine est incorporée dans le risque de base* $h_0$).

En analyse de survie, on modélise donc la fonction $h$ de risque qui dans le cas du modèle de **Cox** correspond au produit de deux quantités :

-   $h_0\geq 0$ : le risque de base qui est fonction uniquement du temps mais indépendant des variables explicatives

-   $e^\eta\geq 0$ : le risque relatif, exponentiel du terme $\eta$ à modéliser qui est fonction des variables explicatives mais indépendant du temps

⚠️Attention, chaque variable explicative dans le modèle de **Cox** doit être [**indépendante du temps**]{.underline} *(sinon, passer au modèle de **Cox étendu**)* et [**non nulles**]{.underline} *(car quand toutes les variables explicatives sont nulles, alors le risque est égal au risque de base car* $e^0=1$*)*⚠️

⚠️Une hypothèse forte du modèle de **Cox** est que les risques sont proportionnels. Il faut donc vérifier que cette hypothèse soit satisfaite. Pour chaque covariable, il faut tester si son effet est indépendant du temps⚠️ *(si ce n'est pas vérifié, on peut* [Stratification] *les covariables qui ne vérifient pas l'hypothèse)*

::: {style="color:red;"}
Avec le modèle de **Cox** dit "semi paramétrique", il n'est pas nécessaire de spécifier le risque de base $h_0$ (elle est la même pour tous les individus à un instant donné) d'où sa grande popularité vis-à-vis des modèles purement paramétriques.
:::

## Risque proportionnel

Le modèle de **Cox** repose sur la notion de [risques proportionnels]{.underline} :

-   Si le test du **log-rank** permet de tester une différence significative de survie entre deux groupes par exemple, il n'est pas possible d'estimer l'étendu de l'impact de cette différence entre ces deux groupes

-   Afin de quantitifer cet impact, on fait appel au [risque instantané]{.underline} de décès de chacun des deux groupes et nous recherchons une fonction simple les reliant

-   Pour y arriver, nous nous basons sur une hypothèse essentielle : nous supposons que la proportion des risques instantanés de décès est constante pendant toute la durée de l'observation, d'où l'expression de ["risque proportionnels"]{.underline}

## Risque proportionnel HR ou hazard ration

Le risque proportionnel (HR) pour une variable $X_i$ est le rapport de deux risques instantanés pour un changement d'une unité (ou catégorie) de $X_i$ tout en maintenant les autres variables $X_{j\neq i}$. Exemple dans le cas où la variable $X_1$ a deux modalités et univarié :

$$ \begin{align*} h(t,X=1)&=h_0(t)\times e^{\beta\times 1}\\ h(t,X=2)&=h_0(t)\times e^{\beta\times 2 }\\ HR &= \frac{h_0(t)\times e^{\beta  \times 1}}{h_0(t)\times e^{\beta_1\times 2 }} = e^{\beta\times(2-1)} = e^{\beta} \end{align*} $$

Dans le cas multivarié :

$$ \begin{align*} h(t,X_1=1,X_{j\neq i} = cste)&=h_0(t)\times e^{\beta_1\times 1+ \dots + \beta_kX_k}\\ h(t,X_1=2,X_{j\neq i} = cste)&=h_0(t)\times e^{\beta_1\times 2 +\dots + \beta_kX_k}\\ HR &= \frac{h_0(t)\times e^{\beta_1\times 1+ \dots + \beta_kX_k}}{h_0(t)\times e^{\beta_1\times 2 +\dots + \beta_kX_k}} = e^{\beta_1\times(2-1)} = e^{\beta_1} \end{align*} $$

À la différence du cas univarié, le coefficient $\beta$ est donné **relativement** au groupe 1.

## Interprétation pour une variable $X_i$

$$ HR=e^{\beta_i}\geq 0 $$

-   $\beta_i = 0 \Rightarrow HR=1$ : pas d'effet de la variable $X_i$ sur le risque global $h$

-   $\beta_i >0 \Rightarrow HR>1$ : augmentation du risque global $h$ lié à la variable $X_i$

-   $\beta_i < 0 \Rightarrow HR<1$ : diminution du risque global $h$ lié à la variable $X_i$

```{r}
variables <- colnames(heart_df)[0:11]

formule <- as.formula(paste("Y ~", paste(variables, collapse = " + ")))

Y = Surv(heart_df$time, heart_df$DEATH_EVENT)
model_cox<- coxph(formule, data = heart_df)
model_cox_summary = summary(model_cox)
model_cox_summary
```

-   `coef` : estimation effet d'une variable sur le risque $h$. Positif : augmentation du risque, négatif : diminution du risque

-   `exp(coef)` : risque proportionnel

-   `se(coef)`: erreur standard

-   `z` : si le coef est significativement différent de 0 : coef/se(coef)

-   `concordance` : capacité prédictive du modèle $\rho_c=\frac{2\rho \sigma_x \sigma_y}{\sigma_x ^2 + \sigma_y ^2 (\mu_x-\mu_y)^2} = 1-\frac{\text{Exp orthogonal squared dist from diagonal x=y}}{\text{Same but assuming independance}}$

-   **Test de Wald** : permet de\
    $$H_0 : \beta_1=\beta_2=\dots =\beta_k =0$$

    De statistique de test :\
    $$W = \beta^T \Sigma ^{-1} \beta$$

    Comme la p value est \< 0.05, alors, on rejette l'hypothèse selon laquelle les coefficients sont tous nuls.

## Interprétation

### Evaluation globale

La concordance étant une valeure entre 0 et 1, on peut se trouver satisfait d'une valeur aussi élevée sans pour autant être trop élevée, ce qui indiquerait que le modèle sur-apprend. Également le test de Wald nous pousse à rejeter significativement le fait que tous les coefficients de régressions sont nuls. Ce qui indique que le modèle parvient à donner des coefficients permettant d'approximer la survie.

### Evaluation par variable

Nous ne pourront conclure que sur les variables pour lesquels le coefficient est significatifs (i.e. p-value inférieure à 0.05). Les variables significatives sont résumées dans ce tableau avec leur influence

```{r}
data <- data.frame(
  Variable = c("age", "ejection_fraction", "serum_creatinine", "anaemia1", 
               "creatinine_phosphokinase", "high_blood_pressure1"),
  Coef_Sign = c("⬆", "⬇", "⬆", "⬆⬆", "⮕", "⬆⬆"),
  Significativite = c("•••", "•••", "•••", "•", "•", "•")
)

data <- data[order(factor(data$Significativite, levels = c("***", "**", "*", ".", "")), decreasing = FALSE), ]

data %>%
  kbl(col.names = c("Variable", "Influence", "Significativité"),
      caption = "Résumé des coefficients et leur significativité") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

## Hypothèse des risques proportionnels

(l'étape suivant est a but pédagogique, en pratique, il faudrait vérifier cette hypothèse avant de faire tourner le modèle de Cox)

Comme précisé précédemment, on doit vérifier l'hypothèse d'indépendance au temps des covariables. En théorie, on a le test suivant :

$$H_0 : \beta_j(t)=\beta_j\quad \forall t$$

En d'autre terme, on veut tester si, pour une covariable $j$, son coefficient de regression $\beta_j$ reste constant sur toute la durée de l'étude. En pratique,

```{r}
test_schoenfeld <- cox.zph(model_cox)


# Sélectionner les deux graphiques à afficher
par(mfrow = c(1, 2))  # Organisation en 1 ligne et 2 colonnes
plot(test_schoenfeld[1], main = "Résidus Schoenfeld - Age")  # Premier graphique
plot(test_schoenfeld[2], main = "Résidus Schoenfeld - Anaemia")  # Deuxième graphique

# Réinitialiser la disposition des graphiques
par(mfrow = c(1, 1))

print(test_schoenfeld)
```

La p-value associée à chaque variable nous montre qu'on ne peut pas rejetter $H_0$ pour la plupart des variables au seuil de significativité de $5\%$, sauf pour ejection_fraction. D'après l'avie médical accompagnant ce rapport, c'est assez normal. En effet, un patient atteint d'insuffisance cardiaque aura une fraction d'ejection instable dans le temps. D'où le fait que cette variable apparaisse comme ayant une saisonalité.

## Sélection de variable

Le but est de maintenant réduire le nombre de variables du modèle. Pour cela, on peut faire un step AIC, le principe est de donner le score global du modèle avec l'AIC et de comparer ce score à celui des variables. On choisi alors de retirer les variables dont l'AIC est supérieur à celui du modèle global.

```{r}
coxaic = stepAIC(model_cox)
```

```{r}
coxbic = stepAIC(model_cox, k = log(nrow(heart_df)))
```

En utilisant le critère AIC, le score obtenu sur le modèle final est 951.83, les variables selectionnées sont :

-   serum_sodium

-   creatinine_phosphokinase

-   anaemia

-   high_blood_pressure

-   serum_creatinine

-   age

-   ejection_fraction.

Le BIC, lui, pour un score global de 969, ne selectionne que les covariables suivantes :

-   serum_creatinine

-   l'age

-   l'ejection fraction

On peut se rassurer que les variables sorties par le stepAIC qui utilise le BIC est : plus restrictif et donne des variables qui sont un sous ensemble que le modèle avec l'AIC.

# Machine learning

Maintenant que l'on a fait une analyse statistique, on propose une rapide analyse prédictive

```{r, warning=FALSE, echo=FALSE, message=FALSE}
# Diviser en jeu d'entraînement et de test
set.seed(123)
library(caret)

trainIndex <- createDataPartition(heart_df$DEATH_EVENT, p = 0.7, list = FALSE)

heart_train <- heart_df[trainIndex, ]
heart_test <- heart_df[-trainIndex, ]
# Modèle d'arbre sans la variable "time"
tree_model <- rpart(DEATH_EVENT ~ ., data = heart_train, method = "class")

# Visualisation de l'arbre avec des détails supplémentaires
rpart.plot(
  tree_model, 
  type = 3, 
  extra = 104, 
  under = TRUE, 
  faclen = 0, 
  cex = 0.8, 
  tweak = 1.2, 
  fallen.leaves = TRUE, 
  box.palette = "RdBu", 
  shadow.col = "gray", 
  main = "Arbre de décision : Prédiction des décès"
)


```

```{r}
pred <- predict(tree_model, heart_test, type = "class")
library(pROC)
prob_pred <- predict(tree_model, heart_test, type = "prob")[, 2]
roc_curve <- roc(heart_test$DEATH_EVENT, prob_pred)
plot(roc_curve, main = "Courbe ROC")
auc(roc_curve)

```

## Avis médical

Tout au long de cette étude, les conclusions ont été cohérente avec la connaissance actuelle de la médecine. Nous tenons néanmoins à ajouter quelques éléments de compréhension :

-   L'insuffisance cardiaque est bien une complication, au vue de sa prévalence et de son importance ce qui implique que l'on ne meurt pas dirrectement

-   Lorsqu'une insuffisance cardiaque se déclare, tout se joue dans les 3 mois avec mise d'une "life veste", si la fraction d'éjection reste trop basse, d'un pacemaker.

-   La fraction d'ejection est un facteur protecteur =\> normal qu'il baisse la mortalité (fraceject ++ =\> eff du \<3 ++ =\> mortalité - - , fractejection - - =\> eff\<3 - - =\> mortalité ++)

-   Le tabac est un indicateur mais il manque de granularité: nbpaquetAnnee, type de cigarette....

# Conclusion

Durant cette étude, nous avons estimer la survie de patients qui présente une insuffisance cardiaque avec l'estimateur de Kaplan Meier et étudiant un cas de covariable en particulier.

Ensuite, nous avons cherchons à approcher cette estimation par une loi de probabilité connu, ce fut sans succès franc, d'autant plus que nous avons découvert un problème dans la librairie R.

Aussi, nous avons étudier l'influence des covariable sur la survie avec le modèle de Cox. Les coefficients finaux et leurs intervalles de confiances sont résumés ci-dessous :

```{r}
df <- data.frame(name = rownames(model_cox_summary$coefficients),   
                 beta = model_cox_summary$coefficients[, "coef"],   
                 se = model_cox_summary$coefficients[, "se(coef)"],
                 pval = model_cox_summary$coefficients[,"Pr(>|z|)"]) 

ggforestplot::forestplot(df = df, name = name,   estimate = beta,   se = se,   pvalue = pval,   psignif = 0.05 )
```

Tous les coefficients qui sont au dessus de $0$ augemente sensiblement le risque de décès d'un patient. On remarque alors que l'anemie, la pression artérielle une créatinine élevée (et le tabac, le diabète et l'âge dans une moindre mesure) augemente réellement ce risque.

Enfin, nous avons, fait une sélection de variable pour afiner le modèle de Cox fait ci-dessus.

Ce qui nous pouvons ajouter à l'étude sont :

-   L'analyse de Cox avec les variables sélectionnées.

-   Entrer plus en détail dans l'analyse prédictive

# Références
